"use strict";(self.webpackChunktaichi_js_com=self.webpackChunktaichi_js_com||[]).push([[3978],{3905:(e,t,n)=>{n.d(t,{Zo:()=>d,kt:()=>m});var r=n(7294);function i(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function a(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?a(Object(n),!0).forEach((function(t){i(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):a(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,r,i=function(e,t){if(null==e)return{};var n,r,i={},a=Object.keys(e);for(r=0;r<a.length;r++)n=a[r],t.indexOf(n)>=0||(i[n]=e[n]);return i}(e,t);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(r=0;r<a.length;r++)n=a[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(i[n]=e[n])}return i}var l=r.createContext({}),p=function(e){var t=r.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},d=function(e){var t=p(e.components);return r.createElement(l.Provider,{value:t},e.children)},c="mdxType",h={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},u=r.forwardRef((function(e,t){var n=e.components,i=e.mdxType,a=e.originalType,l=e.parentName,d=s(e,["components","mdxType","originalType","parentName"]),c=p(n),u=i,m=c["".concat(l,".").concat(u)]||c[u]||h[u]||a;return n?r.createElement(m,o(o({ref:t},d),{},{components:n})):r.createElement(m,o({ref:t},d))}));function m(e,t){var n=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var a=n.length,o=new Array(a);o[0]=u;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s[c]="string"==typeof e?e:i,o[1]=s;for(var p=2;p<a;p++)o[p]=n[p];return r.createElement.apply(null,o)}return r.createElement.apply(null,n)}u.displayName="MDXCreateElement"},5435:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>a,metadata:()=>s,toc:()=>p});var r=n(7462),i=(n(7294),n(3905));const a={sidebar_position:1,title:"Render Pipelines"},o=void 0,s={unversionedId:"docs/rendering/render-pipelines",id:"docs/rendering/render-pipelines",title:"Render Pipelines",description:"Perhaps the biggest difference between taichi.js and the Python taichi library is that taichi.js includes support for writing rendering pipelines composed of vertex and fragment shaders. Writing rendering code in taichi.js is slightly more involved than writing general-purpose compute kernels, and it does require some understanding of rasterization-based rendering in general. However, you will find that the simple programming model of taichi.js makes these concepts extremely easy to work with and reason about.",source:"@site/docs/docs/3-rendering/1-render-pipelines.md",sourceDirName:"docs/3-rendering",slug:"/docs/rendering/render-pipelines",permalink:"/docs/docs/rendering/render-pipelines",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/docs/3-rendering/1-render-pipelines.md",tags:[],version:"current",sidebarPosition:1,frontMatter:{sidebar_position:1,title:"Render Pipelines"},sidebar:"tutorialSidebar",previous:{title:"Class Kernels",permalink:"/docs/docs/advanced/class-kernels"},next:{title:"Textures",permalink:"/docs/docs/rendering/textures"}},l={},p=[],d={toc:p},c="wrapper";function h(e){let{components:t,...n}=e;return(0,i.kt)(c,(0,r.Z)({},d,n,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("p",null,"Perhaps the biggest difference between ",(0,i.kt)("inlineCode",{parentName:"p"},"taichi.js")," and the Python ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/taichi-dev/taichi"},(0,i.kt)("inlineCode",{parentName:"a"},"taichi")," library")," is that ",(0,i.kt)("inlineCode",{parentName:"p"},"taichi.js")," includes support for writing rendering pipelines composed of vertex and fragment shaders. Writing rendering code in ",(0,i.kt)("inlineCode",{parentName:"p"},"taichi.js")," is slightly more involved than writing general-purpose compute kernels, and it does require some understanding of rasterization-based rendering in general. However, you will find that the simple programming model of taichi.js makes these concepts extremely easy to work with and reason about. "),(0,i.kt)("p",null,"In this page, we will go through the basics of creating a render pipeline by rendering a spinning 3D cube. The full source code of this example can be found ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/AmesingFlank/taichi.js/blob/master/examples/rotating-cube/index.js"},"here"),", or alternatively for an interactive version that you can play with, ",(0,i.kt)("a",{parentName:"p",href:"https://taichi-js.com/playground/rotating-cube"},"here"),"."),(0,i.kt)("h1",{id:"vertex-buffer-and-index-buffer"},"Vertex Buffer and Index Buffer"),(0,i.kt)("p",null,'In rasterization-based render pipelines, the geometry to be rendered are always represented as a collection of triangles. In our case, the cube we are trying to render contains 6 faces, each consisting of 2 triangles, giving 12 triangles in total. One way to represent these triangles would be to list all of their vertex positions in a field, which is called a "vertex buffer": '),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-js"},"let vertices = ti.field(ti.types.vector(ti.f32, 3), 36);\nawait vertices.fromArray([\n    [0, 0, 0], // 1st triangle, 1st vertex\n    [0, 0, 1], // 1st triangle, 2nd vertex\n    [0, 1, 0], // 1st triangle, 3rd vertex\n    [0, 0, 1], // 2nd triangle, 1st vertex\n    [0, 1, 1], // 2nd triangle, 2nd vertex\n    [0, 1, 0], // 2nd triangle, 3rd vertex\n    ...\n])\n")),(0,i.kt)("p",null,"Even though there are 12 triangles to be rendered and thus 36 vertices to be declared, the cube itself only has 8 vertices, each of which are shared among multiple triangles. For this reason, it is more efficient to store the positions of each unique vertex in each field, and use a different field to represent which vertices make up each of the 12 triangles:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-js"},"let vertices = ti.field(ti.types.vector(ti.f32, 3), 8);\nlet indices = ti.field(ti.types.vector(ti.i32, 3), 12);\n\nawait vertices.fromArray([\n    [0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1], \n    [1, 0, 0], [1, 0, 1], [1, 1, 0], [1, 1, 1],\n]);\nawait indices.fromArray([\n    [0, 1, 2], [1, 3, 2], [4, 5, 6], [5, 7, 6], \n    [0, 2, 4], [2, 6, 4], [1, 3, 5], [3, 7, 5], \n    [0, 1, 4], [1, 5, 4], [2, 3, 6], [3, 7, 6],\n]);\n\nti.addToKernelScope({ vertices, indices })\n")),(0,i.kt)("p",null,'The 2nd buffer is known as an "index buffer", as it contains indices to the vertex buffer field. In 3D rendering, vertex buffers are almost always used together with an index buffer, although using a vertex buffer on its own is still allowed.'),(0,i.kt)("p",null,"In our cube example, the 3D model is simple enough that we can hard-code its vertex and index buffers by hand. However, for rendering complex scense, you will almost always populate these buffers by importing data from a 3D file format. In ",(0,i.kt)("inlineCode",{parentName:"p"},"taichi.js"),", there are some built-in utilities that help you import GLTF formats. Details can be found ",(0,i.kt)("a",{parentName:"p",href:"https://taichi-js.com/docs/docs/rendering/GLTF"},"here"),"."),(0,i.kt)("h1",{id:"render-target"},"Render Target"),(0,i.kt)("p",null,"Before drawing anything, we need access to a piece of canvas that we are drawing onto. Assuming that a canvas named ",(0,i.kt)("inlineCode",{parentName:"p"},"result_canvas")," exists in the HTML, the following lines of code creates a ",(0,i.kt)("inlineCode",{parentName:"p"},"ti.CanvasTexture")," object, which represents a piece of texture that can be rendered onto by a ",(0,i.kt)("inlineCode",{parentName:"p"},"taichi.js")," render pipeline. "),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-js"},"let htmlCanvas = document.getElementById('result_canvas');\nlet renderTarget = ti.canvasTexture(htmlCanvas);\nti.addToKernelScope({ renderTarget })\n")),(0,i.kt)("h1",{id:"depth-buffer"},"Depth Buffer"),(0,i.kt)("p",null,"In 3D rendering, an important piece of data is the ",(0,i.kt)("inlineCode",{parentName:"p"},"Depth Buffer"),". This is an image which is used by render pipelines to ensure that objects closer to the camera are rendered over further ones. The depth buffer should have the same dimensions as the render target:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-js"},"let depthBuffer = ti.depthTexture(renderTarget.dimensions);\nti.addToKernelScope({ depthBuffer });\n")),(0,i.kt)("h1",{id:"vertex-and-fragment-shaders"},"Vertex and Fragment Shaders"),(0,i.kt)("p",null,"We have now created all the necessary data needed to render our cube, and can start defining the actual render pipeline. Here's the complete code for this pipeline:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-js",metastring:"showLineNumbers",showLineNumbers:!0},"let render = ti.kernel((t) => {\n    let center = [0.5, 0.5, 0.5];\n    let eye = center + [ti.sin(t), 0.5, ti.cos(t)] * 2;\n    let view = ti.lookAt(eye, center, [0.0, 1.0, 0.0]);\n\n    let aspectRatio = renderTarget.dimensions[0] / renderTarget.dimensions[1];\n    let projection = ti.perspective(/* fov */45.0, aspectRatio, /* near */0.1, /* far */100);\n\n    let viewProjection = proj.matmul(view);\n\n    ti.clearColor(renderTarget, [0.1, 0.2, 0.3, 1]);\n    ti.useDepth(depthBuffer);\n\n    for (let v of ti.inputVertices(vertices, indices)) {\n        let pos = viewProjection.matmul(v.concat([1.0]));\n        ti.outputPosition(pos);\n        ti.outputVertex(v);\n    }\n    for (let f of ti.inputFragments()) {\n        let color = f.concat([1.0]);\n        ti.outputColor(renderTarget, color);\n    }\n});\n")),(0,i.kt)("p",null,"The render pipeline takes as input an argument ",(0,i.kt)("inlineCode",{parentName:"p"},"t"),", which has type ",(0,i.kt)("inlineCode",{parentName:"p"},"ti.f32"),' by default. This argument represents "time", and is passed from CPU to control the rotation of the cube. Specifically, the time is used to compute the ',(0,i.kt)("inlineCode",{parentName:"p"},"eye")," variable, which specifies the position from which we are observing the scene. Our pipeline will render the scene from the perspective of this position. The ",(0,i.kt)("inlineCode",{parentName:"p"},"center")," variable specifies the center of the cube, and the expression ",(0,i.kt)("inlineCode",{parentName:"p"},"center + [ti.sin(t), 0.5, ti.cos(t)] * 2")," makes the camera rotate around the cube's center. From the ",(0,i.kt)("inlineCode",{parentName:"p"},"eye")," and ",(0,i.kt)("inlineCode",{parentName:"p"},"center")," variable, we create a ",(0,i.kt)("inlineCode",{parentName:"p"},"view"),' matrix, which transforms a 3D location from "world space" to "view space". The ',(0,i.kt)("inlineCode",{parentName:"p"},"view")," matrix and ",(0,i.kt)("inlineCode",{parentName:"p"},"lookAt()")," function are common computer graphics functions. If you are unfamiliar with these concepts, you may read more about them ",(0,i.kt)("a",{parentName:"p",href:"https://learnopengl.com/Getting-started/Coordinate-Systems"},"here")," and ",(0,i.kt)("a",{parentName:"p",href:"https://learnopengl.com/Getting-started/Camera"},"here"),"."),(0,i.kt)("p",null,"The next matrix we compute is the ",(0,i.kt)("inlineCode",{parentName:"p"},"projection"),' matrix, which transforms a position from "view space" to "clip space". This is computed using a fixed field-of-view angle, the aspect ratio of the render target, and fixed near- and far- distances. The ',(0,i.kt)("inlineCode",{parentName:"p"},"projection")," matrix is multiplied with the ",(0,i.kt)("inlineCode",{parentName:"p"},"viewProject")," matrix, which is used later within the vertex shader. Notice that the ",(0,i.kt)("inlineCode",{parentName:"p"},"view"),", ",(0,i.kt)("inlineCode",{parentName:"p"},"projection"),", and ",(0,i.kt)("inlineCode",{parentName:"p"},"viewProjection")," matrices are all computed in the sequential section of this kernel.This means that these computation are done in a single WebGPU thread, which prepares these matrices before the rendering starts. "),(0,i.kt)("p",null,"After the matrices are computed, we have a ",(0,i.kt)("inlineCode",{parentName:"p"},"ti.clearColor")," call, which specifies the color to fill the render target. This will be the background color of our canvas. Then, we have a ",(0,i.kt)("inlineCode",{parentName:"p"},"ti.useDepth")," call, which declares the depth buffer to be used for depth testing."),(0,i.kt)("p",null,"The core of the render pipeline are represented in two ",(0,i.kt)("inlineCode",{parentName:"p"},"for")," loops. The first loop iterates over ",(0,i.kt)("inlineCode",{parentName:"p"},"ti.inputVertices(vertices, indices)"),", and represents the vertex shader. The second loop iterates over ",(0,i.kt)("inlineCode",{parentName:"p"},"ti.inputFragments()"),", representing the fragment shader. "),(0,i.kt)("p",null,"In the vertex shader, the type of the loop index variable ",(0,i.kt)("inlineCode",{parentName:"p"},"v")," is the same as the element type of the vertex buffer. In our case, the vertex buffer ",(0,i.kt)("inlineCode",{parentName:"p"},"vertices")," has element type ",(0,i.kt)("inlineCode",{parentName:"p"},"ti.types.vector(ti.f32, 3)"),", so ",(0,i.kt)("inlineCode",{parentName:"p"},"v")," is also of this type. Since ",(0,i.kt)("inlineCode",{parentName:"p"},"v")," represents the world-space position of each vertex in the 3D mesh, we transform it using the ",(0,i.kt)("inlineCode",{parentName:"p"},"viewProjection")," matrix into clip space, which we output using the ",(0,i.kt)("inlineCode",{parentName:"p"},"ti.outputPosition(..)")," call. The vertex shader also passes the ",(0,i.kt)("inlineCode",{parentName:"p"},"v")," variable in ",(0,i.kt)("inlineCode",{parentName:"p"},"ti.outputVertex(..)"),", which asks the GPU to interpolate the ",(0,i.kt)("inlineCode",{parentName:"p"},"v")," variable and passes the interpolated values into the fragment shader."),(0,i.kt)("p",null,"In the fragment shader, the loop index variable is ",(0,i.kt)("inlineCode",{parentName:"p"},"f"),". This variable will have the same type as the variable output by the vertex shader. In our case, the vertex shader outputs ",(0,i.kt)("inlineCode",{parentName:"p"},"v"),", which has type ",(0,i.kt)("inlineCode",{parentName:"p"},"ti.types.vector(ti.f32, 3)"),", so ",(0,i.kt)("inlineCode",{parentName:"p"},"f")," will also have this type. For each fragment, the value of ",(0,i.kt)("inlineCode",{parentName:"p"},"f")," will be the interpolated value of ",(0,i.kt)("inlineCode",{parentName:"p"},"v")," across the 3 vertices of the triangle that contains the fragment. Since ",(0,i.kt)("inlineCode",{parentName:"p"},"v")," represents the 3D position of each vertex, the variable ",(0,i.kt)("inlineCode",{parentName:"p"},"f")," will also represent the 3D position of each fragment. We interpret this position as a RGB color value and output it using ",(0,i.kt)("inlineCode",{parentName:"p"},"ti.outputColor(..)"),". As a result, the ",(0,i.kt)("inlineCode",{parentName:"p"},"x")," value of the fragment's location will determine how red it is painted, and the ",(0,i.kt)("inlineCode",{parentName:"p"},"y")," and ",(0,i.kt)("inlineCode",{parentName:"p"},"z")," values will determine how green and blue it is. "),(0,i.kt)("p",null,"We invoke the pipeline every frame with an increasing value of ",(0,i.kt)("inlineCode",{parentName:"p"},"t"),":"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-js"},"let i = 0;\nasync function frame() {\n    render(i * 0.03);\n    i = i + 1;\n    requestAnimationFrame(frame);\n}\nrequestAnimationFrame(frame);\n")),(0,i.kt)("p",null,"which renders the rotating colored cube we expected:"),(0,i.kt)("h1",{id:"structured-vertices-and-interpolants"},"Structured vertices and Interpolants"))}h.isMDXComponent=!0}}]);