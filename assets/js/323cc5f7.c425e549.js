"use strict";(self.webpackChunktaichi_js_com=self.webpackChunktaichi_js_com||[]).push([[4038],{3905:(e,t,r)=>{r.d(t,{Zo:()=>u,kt:()=>g});var a=r(7294);function n(e,t,r){return t in e?Object.defineProperty(e,t,{value:r,enumerable:!0,configurable:!0,writable:!0}):e[t]=r,e}function i(e,t){var r=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),r.push.apply(r,a)}return r}function l(e){for(var t=1;t<arguments.length;t++){var r=null!=arguments[t]?arguments[t]:{};t%2?i(Object(r),!0).forEach((function(t){n(e,t,r[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(r)):i(Object(r)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(r,t))}))}return e}function o(e,t){if(null==e)return{};var r,a,n=function(e,t){if(null==e)return{};var r,a,n={},i=Object.keys(e);for(a=0;a<i.length;a++)r=i[a],t.indexOf(r)>=0||(n[r]=e[r]);return n}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(a=0;a<i.length;a++)r=i[a],t.indexOf(r)>=0||Object.prototype.propertyIsEnumerable.call(e,r)&&(n[r]=e[r])}return n}var s=a.createContext({}),p=function(e){var t=a.useContext(s),r=t;return e&&(r="function"==typeof e?e(t):l(l({},t),e)),r},u=function(e){var t=p(e.components);return a.createElement(s.Provider,{value:t},e.children)},d="mdxType",c={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},m=a.forwardRef((function(e,t){var r=e.components,n=e.mdxType,i=e.originalType,s=e.parentName,u=o(e,["components","mdxType","originalType","parentName"]),d=p(r),m=n,g=d["".concat(s,".").concat(m)]||d[m]||c[m]||i;return r?a.createElement(g,l(l({ref:t},u),{},{components:r})):a.createElement(g,l({ref:t},u))}));function g(e,t){var r=arguments,n=t&&t.mdxType;if("string"==typeof e||n){var i=r.length,l=new Array(i);l[0]=m;var o={};for(var s in t)hasOwnProperty.call(t,s)&&(o[s]=t[s]);o.originalType=e,o[d]="string"==typeof e?e:n,l[1]=o;for(var p=2;p<i;p++)l[p]=r[p];return a.createElement.apply(null,l)}return a.createElement.apply(null,r)}m.displayName="MDXCreateElement"},7384:(e,t,r)=>{r.r(t),r.d(t,{assets:()=>s,contentTitle:()=>l,default:()=>c,frontMatter:()=>i,metadata:()=>o,toc:()=>p});var a=r(7462),n=(r(7294),r(3905));const i={sidebar_position:2,title:"Textures"},l=void 0,o={unversionedId:"docs/rendering/textures",id:"docs/rendering/textures",title:"Textures",description:"Textures are an important concept in computer graphics. They are images that come with hardware-accelerated sampling and filtering, and can be used to add details to 3D geometry. This page documents texture support in taichi.js.",source:"@site/docs/docs/3-rendering/2-textures.md",sourceDirName:"docs/3-rendering",slug:"/docs/rendering/textures",permalink:"/docs/docs/rendering/textures",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/docs/3-rendering/2-textures.md",tags:[],version:"current",sidebarPosition:2,frontMatter:{sidebar_position:2,title:"Textures"},sidebar:"tutorialSidebar",previous:{title:"Render Pipelines",permalink:"/docs/docs/rendering/render-pipelines"},next:{title:"Rendering Built-in Functions",permalink:"/docs/docs/rendering/rendering-builtin-functions"}},s={},p=[{value:"Creating Textures",id:"creating-textures",level:2},{value:"Loading Texture from An Image",id:"loading-texture-from-an-image",level:2},{value:"Sampling from a Texture",id:"sampling-from-a-texture",level:2},{value:"3D Textures",id:"3d-textures",level:2},{value:"Cubemaps",id:"cubemaps",level:2},{value:"Depth Textures",id:"depth-textures",level:2},{value:"Render Targets",id:"render-targets",level:2},{value:"MSAA Textures",id:"msaa-textures",level:2},{value:"Reading and Writing Texels",id:"reading-and-writing-texels",level:2}],u={toc:p},d="wrapper";function c(e){let{components:t,...r}=e;return(0,n.kt)(d,(0,a.Z)({},u,r,{components:t,mdxType:"MDXLayout"}),(0,n.kt)("p",null,"Textures are an important concept in computer graphics. They are images that come with hardware-accelerated sampling and filtering, and can be used to add details to 3D geometry. This page documents texture support in ",(0,n.kt)("inlineCode",{parentName:"p"},"taichi.js"),"."),(0,n.kt)("p",null,"For examples of complete ",(0,n.kt)("inlineCode",{parentName:"p"},"taichi.js")," programs that make use of textures, see"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://taichi-js.com/playground/fractal-on-cloth"},"Fractal Cloth")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://taichi-js.com/playground/gltf-pbr"},"GLTF Physically-Based Rendering"))),(0,n.kt)("h2",{id:"creating-textures"},"Creating Textures"),(0,n.kt)("p",null,"A standard 2D texture with RGBA components can be created using"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-js"},"let texture = ti.texture(4, [width, height]);\n")),(0,n.kt)("p",null,"The argument 4 specifies the amount of components in each texel. You can also pass 1 here for red-only, or 2 for red and green only."),(0,n.kt)("h2",{id:"loading-texture-from-an-image"},"Loading Texture from An Image"),(0,n.kt)("p",null,"You can create an image from either an ",(0,n.kt)("a",{parentName:"p",href:"https://developer.mozilla.org/en-US/docs/Web/API/ImageBitmap"},(0,n.kt)("inlineCode",{parentName:"a"},"ImageBitmap")),", an ",(0,n.kt)("a",{parentName:"p",href:"https://developer.mozilla.org/en-US/docs/Web/API/HTMLImageElement"},(0,n.kt)("inlineCode",{parentName:"a"},"HTMLImageElement")),", or simply from an url of the image. You can use these APIs:"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-js"},"let texture = await ti.Texture.createFromBitmap(bitmap);\nlet texture = await ti.Texture.createFromHtmlImage(htmlImageElement);\nlet texture = await ti.Texture.createFromURL(url);\n")),(0,n.kt)("h2",{id:"sampling-from-a-texture"},"Sampling from a Texture"),(0,n.kt)("p",null,"Given a texture object, you can sample from it in a kernel:"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-js"},"let texel = ti.textureSample(tex, texCoords);\n")),(0,n.kt)("p",null,"Here, the first argument is the texture object itself (either exposed to kernel scope via ",(0,n.kt)("inlineCode",{parentName:"p"},"ti.addToKernelScope(..)")," or passed as ",(0,n.kt)("inlineCode",{parentName:"p"},"ti.template()")," argument). The 2nd argument is the texture coordinates, which should be a 2D vector for 2D textures, or a 3D vector for 3D textures and cubemaps."),(0,n.kt)("h2",{id:"3d-textures"},"3D Textures"),(0,n.kt)("p",null,"3D textures can be created by providing an extra ",(0,n.kt)("inlineCode",{parentName:"p"},"depth")," value when calling ",(0,n.kt)("inlineCode",{parentName:"p"},"ti.texture"),":"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-js"},"let texture = ti.texture(4, [width, height, depth]);\n")),(0,n.kt)("h2",{id:"cubemaps"},"Cubemaps"),(0,n.kt)("p",null,"A cubemap is a collection of six 2D textures, each representing one side of a cube. In computer graphics, cubemaps are often used to represent an environment map. Read more about cubemaps ",(0,n.kt)("a",{parentName:"p",href:"https://learnopengl.com/Advanced-OpenGL/Cubemaps"},"here"),". In ",(0,n.kt)("inlineCode",{parentName:"p"},"taichi.js"),", you can create a cubemap using an array of six ",(0,n.kt)("a",{parentName:"p",href:"https://developer.mozilla.org/en-US/docs/Web/API/ImageBitmap"},(0,n.kt)("inlineCode",{parentName:"a"},"ImageBitmap"),"s"),", ",(0,n.kt)("a",{parentName:"p",href:"https://developer.mozilla.org/en-US/docs/Web/API/HTMLImageElement"},(0,n.kt)("inlineCode",{parentName:"a"},"HTMLImageElement"),"s"),", or urls:"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-js"},"let cubemap = await ti.CubeTexture.createFromURL([\n    urlRight,\n    urlLeft,\n    urlTop,\n    urlBottom,\n    urlFront,\n    urlBack,\n]);\n")),(0,n.kt)("h2",{id:"depth-textures"},"Depth Textures"),(0,n.kt)("p",null,"As ",(0,n.kt)("a",{parentName:"p",href:"https://taichi-js.com/docs/docs/rendering/render-pipelines#depth-buffer"},"previously mentioned"),", a depth buffer can be created by"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-js"},"let depthBuffer = ti.depthTexture(renderTarget.dimensions);\n")),(0,n.kt)("p",null,"In render pipelines, you may use depth buffers for depth testing by specifying"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-js"},"ti.useDepth(depthBuffer);\n")),(0,n.kt)("p",null,"But for algorithms such as ",(0,n.kt)("a",{parentName:"p",href:"https://learnopengl.com/Advanced-Lighting/Shadows/Shadow-Mapping"},"Shadow Maps"),', you can explicitly carry out "compare-and-sample" operations on depth buffers:'),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-js"},"let visibility = ti.textureSampleCompare(shadowMap, coords, depth);\n")),(0,n.kt)("h2",{id:"render-targets"},"Render Targets"),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"https://taichi-js.com/docs/docs/rendering/render-pipelines#render-target"},"Previously")," we showed how you can create a render target texture which corresponds to an HTML canvas:"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-js"},"let htmlCanvas = document.getElementById('result_canvas');\nlet renderTarget = ti.canvasTexture(htmlCanvas);\n")),(0,n.kt)("p",null,"But you can also create render targets that are not tied to any HTML canvases. This is helpful if your renderer contains multiple passes, and you need renderTargets to store intermediate results (e.g. ",(0,n.kt)("a",{parentName:"p",href:"https://learnopengl.com/Advanced-Lighting/Deferred-Shading"},"GBuffers"),"). You can do this directly by declaring a texture and using it as the render output in your pipeline:"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-js"},"let gBuffer = ti.texture(4, [width, height]);\n...\nlet render = ti.kernel((..) => {\n    ti.clearColor(gBuffer, ...);\n    for (let v of ti.inputVertices(...)) {\n        ...\n    }\n    for (let f of ti.inputFragments()) {\n        ti.outputColor(gBuffer, ...);\n    }\n})\n")),(0,n.kt)("h2",{id:"msaa-textures"},"MSAA Textures"),(0,n.kt)("p",null,"MSAA, or ",(0,n.kt)("a",{parentName:"p",href:"https://learnopengl.com/Advanced-OpenGL/Anti-Aliasing"},"Multi-Sampled Anti Aliasing"),", helps reduce jagged edges at the boundaries of rendered objects. In ",(0,n.kt)("inlineCode",{parentName:"p"},"taichi.js"),", you can enable MSAA by providing an optional sample count at the end of your render targets and depth buffers. In the following example, the renderTarget, gBuffer, and depth texture, all have 4xMSAA:"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-js"},"let htmlCanvas = document.getElementById('result_canvas');\nlet renderTarget = ti.canvasTexture(htmlCanvas, 4);\nlet depthTexture = ti.depthTexture( [htmlCanvas.width, htmlCanvas.height], 4);\nlet gBuffer = ti.texture(4, [htmlCanvas.width, htmlCanvas.height], 4);\n")),(0,n.kt)("h2",{id:"reading-and-writing-texels"},"Reading and Writing Texels"),(0,n.kt)("p",null,(0,n.kt)("inlineCode",{parentName:"p"},"taichi.js")," allows you to read and write the values of individual texels, given the integer coordinates of the texel. These can be achieved using the following functions:"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-js"},"let texelValue = ti.textureLoad(tex, texelCoords);\nti.textureStore(tex, texelCoords, texelValue)\n")),(0,n.kt)("p",null,"As an example, the ",(0,n.kt)("a",{parentName:"p",href:"https://taichi-js.com/playground/fractal-on-cloth"},"Fractal Cloth")," program uses a parallel compute pipeline to populate a texture using ",(0,n.kt)("inlineCode",{parentName:"p"},"ti.textureStore\n(..)"),", and samples the texture in a fragment shader. Similarly, in a more advanced scenario, the ",(0,n.kt)("a",{parentName:"p",href:"https://taichi-js.com/playground/gltf-pbr"},"GLTF Physically-Based Rendering")," program uses a compute pipeline to pre-filter environment maps represented as equi-rectangular textures, and sample them during rendering."))}c.isMDXComponent=!0}}]);