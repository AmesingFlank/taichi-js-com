"use strict";(self.webpackChunktaichi_js_com=self.webpackChunktaichi_js_com||[]).push([[3978],{3905:(e,t,n)=>{n.d(t,{Zo:()=>c,kt:()=>f});var r=n(7294);function i(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function a(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?a(Object(n),!0).forEach((function(t){i(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):a(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,r,i=function(e,t){if(null==e)return{};var n,r,i={},a=Object.keys(e);for(r=0;r<a.length;r++)n=a[r],t.indexOf(n)>=0||(i[n]=e[n]);return i}(e,t);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(r=0;r<a.length;r++)n=a[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(i[n]=e[n])}return i}var l=r.createContext({}),d=function(e){var t=r.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},c=function(e){var t=d(e.components);return r.createElement(l.Provider,{value:t},e.children)},p="mdxType",u={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},h=r.forwardRef((function(e,t){var n=e.components,i=e.mdxType,a=e.originalType,l=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),p=d(n),h=i,f=p["".concat(l,".").concat(h)]||p[h]||u[h]||a;return n?r.createElement(f,o(o({ref:t},c),{},{components:n})):r.createElement(f,o({ref:t},c))}));function f(e,t){var n=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var a=n.length,o=new Array(a);o[0]=h;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s[p]="string"==typeof e?e:i,o[1]=s;for(var d=2;d<a;d++)o[d]=n[d];return r.createElement.apply(null,o)}return r.createElement.apply(null,n)}h.displayName="MDXCreateElement"},5435:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>o,default:()=>u,frontMatter:()=>a,metadata:()=>s,toc:()=>d});var r=n(7462),i=(n(7294),n(3905));const a={sidebar_position:1,title:"Render Pipelines"},o=void 0,s={unversionedId:"docs/rendering/render-pipelines",id:"docs/rendering/render-pipelines",title:"Render Pipelines",description:"Perhaps the biggest difference between taichi.js and the Python taichi library is that taichi.js includes support for writing rendering pipelines composed of vertex and fragment shaders. Writing rendering code in taichi.js is slightly more involved than writing general-purpose compute kernels, and it does require some understanding of rasterization-based rendering in general. However, you will find that the simple programming model of taichi.js makes these concepts extremely easy to work with and reason about.",source:"@site/docs/docs/3-rendering/1-render-pipelines.md",sourceDirName:"docs/3-rendering",slug:"/docs/rendering/render-pipelines",permalink:"/docs/docs/rendering/render-pipelines",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/docs/3-rendering/1-render-pipelines.md",tags:[],version:"current",sidebarPosition:1,frontMatter:{sidebar_position:1,title:"Render Pipelines"},sidebar:"tutorialSidebar",previous:{title:"Class Kernels",permalink:"/docs/docs/advanced/class-kernels"},next:{title:"Textures",permalink:"/docs/docs/rendering/textures"}},l={},d=[],c={toc:d},p="wrapper";function u(e){let{components:t,...n}=e;return(0,i.kt)(p,(0,r.Z)({},c,n,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("p",null,"Perhaps the biggest difference between ",(0,i.kt)("inlineCode",{parentName:"p"},"taichi.js")," and the Python ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/taichi-dev/taichi"},(0,i.kt)("inlineCode",{parentName:"a"},"taichi")," library")," is that ",(0,i.kt)("inlineCode",{parentName:"p"},"taichi.js")," includes support for writing rendering pipelines composed of vertex and fragment shaders. Writing rendering code in ",(0,i.kt)("inlineCode",{parentName:"p"},"taichi.js")," is slightly more involved than writing general-purpose compute kernels, and it does require some understanding of rasterization-based rendering in general. However, you will find that the simple programming model of taichi.js makes these concepts extremely easy to work with and reason about. "),(0,i.kt)("p",null,"In this page, we will go through the basics of creating a render pipeline by rendering a spinning 3D cube. The full source code of this example can be found here, or alternatively for an interactive version that you can play with, here."),(0,i.kt)("h1",{id:"vertex-buffer-and-index-buffer"},"Vertex Buffer and Index Buffer"),(0,i.kt)("p",null,'In rasterization-based render pipelines, the geometry to be rendered are always represented as a collection of triangles. In our case, the cube we are trying to render contains 6 faces, each consisting of 2 triangles, giving 12 triangles in total. One way to represent these triangles would be to list all of their vertex positions in a field, which is called a "vertex buffer": '),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-js"},"let vertices = ti.field(ti.types.vector(ti.f32, 3), 36);\nawait vertices.fromArray([\n    [0, 0, 0], // 1st triangle, 1st vertex\n    [0, 0, 1], // 1st triangle, 2nd vertex\n    [0, 1, 0], // 1st triangle, 3rd vertex\n    [0, 0, 1], // 2nd triangle, 1st vertex\n    [0, 1, 1], // 2nd triangle, 2nd vertex\n    [0, 1, 0], // 2nd triangle, 3rd vertex\n    ...\n])\n")),(0,i.kt)("p",null,"Even though there are 12 triangles to be rendered and thus 36 vertices to be declared, the cube itself only has 8 vertices, each of which are shared among multiple triangles. For this reason, it is more efficient to store the positions of each unique vertex in each field, and use a different field to represent which vertices make up each of the 12 triangles:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-js"},"let vertices = ti.field(ti.types.vector(ti.f32, 3), 8);\nlet indices = ti.field(ti.types.vector(ti.i32, 3), 12);\n\nawait vertices.fromArray([\n    [0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1], \n    [1, 0, 0], [1, 0, 1], [1, 1, 0], [1, 1, 1],\n]);\nawait indices.fromArray([\n    [0, 1, 2], [1, 3, 2], [4, 5, 6], [5, 7, 6], \n    [0, 2, 4], [2, 6, 4], [1, 3, 5], [3, 7, 5], \n    [0, 1, 4], [1, 5, 4], [2, 3, 6], [3, 7, 6],\n]);\n\nti.addToKernelScope({ vertices, indices })\n")),(0,i.kt)("p",null,'The 2nd buffer is known as an "index buffer", as it contains indices to the vertex buffer field. In 3D rendering, vertex buffers are almost always used together with an index buffer, although using a vertex buffer on its own is still allowed.'),(0,i.kt)("p",null,"In our cube example, the 3D model is simple enough that we can hard-code its vertex and index buffers by hand. However, for rendering complex scense, you will almost always populate these buffers by importing data from a 3D file format. In ",(0,i.kt)("inlineCode",{parentName:"p"},"taichi.js"),", there are some built-in utilities that help you import GLTF formats. Details can be found here."),(0,i.kt)("h1",{id:"render-target"},"Render Target"),(0,i.kt)("p",null,"Before drawing anything, we need access to a piece of canvas that we are drawing onto. Assuming that a canvas named ",(0,i.kt)("inlineCode",{parentName:"p"},"result_canvas")," exists in the HTML, the following lines of code creates a ",(0,i.kt)("inlineCode",{parentName:"p"},"ti.CanvasTexture")," object, which represents a piece of texture that can be rendered onto by a ",(0,i.kt)("inlineCode",{parentName:"p"},"taichi.js")," render pipeline. "),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-js"},"let htmlCanvas = document.getElementById('result_canvas');\nlet renderTarget = ti.canvasTexture(htmlCanvas);\nti.addToKernelScope({ renderTarget })\n")),(0,i.kt)("h1",{id:"depth-buffer"},"Depth Buffer"),(0,i.kt)("p",null,"In 3D rendering, an important piece of data is the ",(0,i.kt)("inlineCode",{parentName:"p"},"Depth Buffer"),". This is an image which is used by render pipelines to ensure that objects closer to the camera are rendered over further ones. The depth buffer should have the same dimensions as the render target:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-js"},"let depth = ti.depthTexture(renderTarget.dimensions);\nti.addToKernelScope({ depth });\n")),(0,i.kt)("h1",{id:"vertex-and-fragment-shders"},"Vertex and Fragment Shders"),(0,i.kt)("p",null,"We have now created all the necessary data needed to render our cube, and can start defining the actual render pipeline. "),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-js"},"let render = ti.kernel((t) => {\n    let center = [0.5, 0.5, 0.5];\n    let eye = center + [ti.sin(t), 0.5, ti.cos(t)] * 2;\n    let view = ti.lookAt(eye, center, [0.0, 1.0, 0.0]);\n    let aspectRatio = renderTarget.dimensions[0] / renderTarget.dimensions[1];\n    let proj = ti.perspective(45.0, aspectRatio, 0.1, 100);\n    let mvp = proj.matmul(view);\n\n    ti.clearColor(renderTarget, [0.1, 0.2, 0.3, 1]);\n    ti.useDepth(depth);\n\n    for (let v of ti.inputVertices(vertices, indices)) {\n        let pos = mvp.matmul(v.concat([1.0]));\n        ti.outputPosition(pos);\n        ti.outputVertex(v);\n    }\n    for (let f of ti.inputFragments()) {\n        let color = f.concat([1.0]);\n        ti.outputColor(renderTarget, color);\n    }\n});\n")),(0,i.kt)("h1",{id:"structured-vertices-and-interpolants"},"Structured vertices and interpolants"))}u.isMDXComponent=!0}}]);